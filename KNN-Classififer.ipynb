{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b74811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started\n",
      "\n",
      "D:/Major Project/data_preprocessed_python/s01.dat\n",
      "D:/Major Project/data_preprocessed_python/s02.dat\n",
      "D:/Major Project/data_preprocessed_python/s03.dat\n",
      "D:/Major Project/data_preprocessed_python/s04.dat\n",
      "D:/Major Project/data_preprocessed_python/s05.dat\n",
      "D:/Major Project/data_preprocessed_python/s06.dat\n",
      "D:/Major Project/data_preprocessed_python/s07.dat\n",
      "D:/Major Project/data_preprocessed_python/s08.dat\n",
      "D:/Major Project/data_preprocessed_python/s09.dat\n",
      "D:/Major Project/data_preprocessed_python/s10.dat\n",
      "D:/Major Project/data_preprocessed_python/s11.dat\n",
      "D:/Major Project/data_preprocessed_python/s12.dat\n",
      "D:/Major Project/data_preprocessed_python/s13.dat\n",
      "D:/Major Project/data_preprocessed_python/s14.dat\n",
      "D:/Major Project/data_preprocessed_python/s15.dat\n",
      "D:/Major Project/data_preprocessed_python/s16.dat\n",
      "D:/Major Project/data_preprocessed_python/s17.dat\n",
      "D:/Major Project/data_preprocessed_python/s18.dat\n",
      "D:/Major Project/data_preprocessed_python/s19.dat\n",
      "D:/Major Project/data_preprocessed_python/s20.dat\n",
      "D:/Major Project/data_preprocessed_python/s21.dat\n",
      "D:/Major Project/data_preprocessed_python/s22.dat\n",
      "D:/Major Project/data_preprocessed_python/s23.dat\n",
      "D:/Major Project/data_preprocessed_python/s24.dat\n",
      "D:/Major Project/data_preprocessed_python/s25.dat\n",
      "D:/Major Project/data_preprocessed_python/s26.dat\n",
      "D:/Major Project/data_preprocessed_python/s27.dat\n",
      "D:/Major Project/data_preprocessed_python/s28.dat\n",
      "D:/Major Project/data_preprocessed_python/s29.dat\n",
      "D:/Major Project/data_preprocessed_python/s30.dat\n",
      "D:/Major Project/data_preprocessed_python/s31.dat\n",
      "D:/Major Project/data_preprocessed_python/s32.dat\n",
      "\n",
      "Print Successful\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "nLabel, nTrial, nUser, nChannel, nTime  = 4, 40, 32, 40, 8064\n",
    "no_of_users=32\n",
    "def convertData():\n",
    "    print(\"Program started\"+\"\\n\")\n",
    "    fout_data = open(\"D:/Major Project/data/features_raw.dat\",'w')\n",
    "    fout_labels0 = open(\"D:/Major Project/data/labels_0.dat\",'w')\n",
    "    fout_labels1 = open(\"D:/Major Project/data/labels_1.dat\",'w')\n",
    "    fout_labels2 = open(\"D:/Major Project/data/labels_2.dat\",'w')\n",
    "    fout_labels3 = open(\"D:/Major Project/data/labels_3.dat\",'w')\n",
    "    for i in range(no_of_users):  #nUser #4, 40, 32, 40, 8064 4 labels, 40 sample for each user, 32 such user, 40 electrode, 8064*40 features\n",
    "        if(i%1 == 0):\n",
    "            if i < 10:\n",
    "                name = '%0*d' % (2,i+1)\n",
    "            else:\n",
    "                name = i+1\n",
    "        fname = \"D:/Major Project/data_preprocessed_python/s\"+str(name)+\".dat\"     \n",
    "        f = open(fname, 'rb')                 #Read the file in Binary mode\n",
    "        x = pickle.load(f, encoding='latin1')\n",
    "        print(fname)                          \n",
    "    \t\n",
    "        for tr in range(nTrial):\n",
    "            if(tr%1 == 0):\n",
    "                for dat in range(nTime):\n",
    "                    if(dat%32 == 0):\n",
    "                        for ch in range(nChannel):\n",
    "                            fout_data.write(str(x['data'][tr][ch][dat]) + \" \");\n",
    "                fout_labels0.write(str(x['labels'][tr][0]) + \"\\n\");\n",
    "                fout_labels1.write(str(x['labels'][tr][1]) + \"\\n\");\n",
    "                fout_labels2.write(str(x['labels'][tr][2]) + \"\\n\");\n",
    "                fout_labels3.write(str(x['labels'][tr][3]) + \"\\n\");\n",
    "                fout_data.write(\"\\n\");\n",
    "    fout_labels0.close()\n",
    "    fout_labels1.close()\n",
    "    fout_labels2.close()\n",
    "    fout_labels3.close()\n",
    "    fout_data.close()\n",
    "    print(\"\\n\"+\"Print Successful\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convertData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5356a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started\n",
      "\n",
      "D:/Major Project/data_preprocessed_python/s01.dat\n",
      "D:/Major Project/data_preprocessed_python/s02.dat\n",
      "D:/Major Project/data_preprocessed_python/s03.dat\n",
      "D:/Major Project/data_preprocessed_python/s04.dat\n",
      "D:/Major Project/data_preprocessed_python/s05.dat\n",
      "D:/Major Project/data_preprocessed_python/s06.dat\n",
      "D:/Major Project/data_preprocessed_python/s07.dat\n",
      "D:/Major Project/data_preprocessed_python/s08.dat\n",
      "D:/Major Project/data_preprocessed_python/s09.dat\n",
      "D:/Major Project/data_preprocessed_python/s10.dat\n",
      "D:/Major Project/data_preprocessed_python/s11.dat\n",
      "D:/Major Project/data_preprocessed_python/s12.dat\n",
      "D:/Major Project/data_preprocessed_python/s13.dat\n",
      "D:/Major Project/data_preprocessed_python/s14.dat\n",
      "D:/Major Project/data_preprocessed_python/s15.dat\n",
      "D:/Major Project/data_preprocessed_python/s16.dat\n",
      "D:/Major Project/data_preprocessed_python/s17.dat\n",
      "D:/Major Project/data_preprocessed_python/s18.dat\n",
      "D:/Major Project/data_preprocessed_python/s19.dat\n",
      "D:/Major Project/data_preprocessed_python/s20.dat\n",
      "D:/Major Project/data_preprocessed_python/s21.dat\n",
      "D:/Major Project/data_preprocessed_python/s22.dat\n",
      "D:/Major Project/data_preprocessed_python/s23.dat\n",
      "D:/Major Project/data_preprocessed_python/s24.dat\n",
      "D:/Major Project/data_preprocessed_python/s25.dat\n",
      "D:/Major Project/data_preprocessed_python/s26.dat\n",
      "D:/Major Project/data_preprocessed_python/s27.dat\n",
      "D:/Major Project/data_preprocessed_python/s28.dat\n",
      "D:/Major Project/data_preprocessed_python/s29.dat\n",
      "D:/Major Project/data_preprocessed_python/s30.dat\n",
      "D:/Major Project/data_preprocessed_python/s31.dat\n",
      "D:/Major Project/data_preprocessed_python/s32.dat\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "nLabel, nTrial, nUser, nChannel, nTime  = 4, 40, 32, 40, 8064  #10080/80=126*40=5040 features\n",
    "no_of_users=32\n",
    "def sampleFeatures():\n",
    "    print(\"Program started\"+\"\\n\")\n",
    "    fout_data = open('D:/Major Project/data/features_sampled.dat','w')\n",
    "    for i in range(no_of_users):   #nUser  #4, 40, 32, 40, 8064\n",
    "        if(i%1 == 0):\n",
    "            if i < 10:\t\t\t\n",
    "                name = '%0*d' % (2,i+1)\n",
    "            else:\n",
    "                name = i+1\t\t\n",
    "            fname = \"D:/Major Project/data_preprocessed_python/s\"+str(name)+\".dat\"\n",
    "            x = pickle.load(open(fname, 'rb'), encoding='latin1')\t\t\n",
    "            print(fname)\t\t\n",
    "            for tr in range(nTrial):\t\t\t\n",
    "                if(tr%1 == 0):\t\t\t\t\n",
    "                    for dat in range(nTime):\t\t\t\t\t\n",
    "                        if(dat%32 == 0 ):\t\t\t\t\t\t\n",
    "                            for ch in range(nChannel):\t\t\t\t\t\t\t\n",
    "                                fout_data.write(str(ch+1) + \" \");\t\t\t\t\t\t\t\n",
    "                                fout_data.write(str(x['data'][tr][ch][dat]) + \" \");\t\t\t\n",
    "                fout_data.write(\"\\n\");\n",
    "    fout_data.close()\n",
    "    print('Completed')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sampleFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21571f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started\n",
      "\n",
      "Encoded label 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def onehotencoding0():\n",
    "    print(\"Program started\"+\"\\n\")\n",
    "    fout_labels_class = open(\"D:/Major Project/data/label_class_0.dat\",'w')\n",
    "    \n",
    "    with open('D:/Major Project/data/labels_0.dat','r') as f:\n",
    "        for val in f:\n",
    "            if float(val) > 4.5:\n",
    "                fout_labels_class.write(str(1) + \"\\n\");\n",
    "            else:\n",
    "                fout_labels_class.write(str(0) + \"\\n\");\n",
    "                \n",
    "    print(\"Encoded label 0\"+\"\\n\")\n",
    "if __name__ == '__main__':\n",
    "    onehotencoding0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371fa795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started\n",
      "\n",
      "Encoded label 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def onehotencoding1():\n",
    "    print(\"Program started\"+\"\\n\")\n",
    "    fout_labels_class = open(\"D:/Major Project/data/label_class_1.dat\",'w')\n",
    "    \n",
    "    with open('D:/Major Project/data/labels_1.dat','r') as f:\n",
    "        for val in f:\n",
    "            if float(val) > 4.5:\n",
    "                fout_labels_class.write(str(1) + \"\\n\");\n",
    "            else:\n",
    "                fout_labels_class.write(str(0) + \"\\n\");\n",
    "    print(\"Encoded label 1\"+\"\\n\")\n",
    "if __name__ == '__main__':\n",
    "    onehotencoding1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33647155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started\n",
      "\n",
      "Encoded label 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def onehotencoding1():\n",
    "    print(\"Program started\"+\"\\n\")\n",
    "    fout_labels_class = open(\"D:/Major Project/data/label_class_2.dat\",'w')\n",
    "    \n",
    "    with open('D:/Major Project/data/labels_2.dat','r') as f:\n",
    "        for val in f:\n",
    "            if float(val) > 4.5:\n",
    "                fout_labels_class.write(str(1) + \"\\n\");\n",
    "            else:\n",
    "                fout_labels_class.write(str(0) + \"\\n\");\n",
    "    print(\"Encoded label 2\"+\"\\n\")\n",
    "if __name__ == '__main__':\n",
    "    onehotencoding1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331a2504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started\n",
      "\n",
      "Encoded label 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def onehotencoding3():\n",
    "    print(\"Program started\"+\"\\n\")\n",
    "    fout_labels_class = open(\"D:/Major Project/data/label_class_3.dat\",'w')\n",
    "    \n",
    "    with open('D:/Major Project/data/labels_3.dat','r') as f:\n",
    "        for val in f:\n",
    "            if float(val) > 4.5:\n",
    "                fout_labels_class.write(str(1) + \"\\n\");\n",
    "            else:\n",
    "                fout_labels_class.write(str(0) + \"\\n\");\n",
    "    print(\"Encoded label 3\"+\"\\n\")\n",
    "if __name__ == '__main__':\n",
    "    onehotencoding3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b06967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model | Mean of CV | Std. Dev. of CV | Time\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 99>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean cv_result\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[43mcross_validate0\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mcross_validate0\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m     55\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 56\u001b[0m     kfold \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_selection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKFold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     cv_results \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mcross_val_score(model, X_train, y_train, cv\u001b[38;5;241m=\u001b[39mkfold, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m     58\u001b[0m     t \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:435\u001b[0m, in \u001b[0;36mKFold.__init__\u001b[1;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296\u001b[0m, in \u001b[0;36m_BaseKFold.__init__\u001b[1;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffle must be True or False; got \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shuffle))\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shuffle \u001b[38;5;129;01mand\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# None is the default\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting a random_state has no effect since shuffle is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse. You should leave \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state to its default (None), or set shuffle=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m=\u001b[39m n_splits\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle \u001b[38;5;241m=\u001b[39m shuffle\n",
      "\u001b[1;31mValueError\u001b[0m: Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def cross_validate0():\n",
    "    \n",
    "    # Get Data\n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_0.dat'\n",
    "    \n",
    "    X = np.genfromtxt(file_x, delimiter=' ')\n",
    "    y = np.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "    \n",
    "     # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "     #principle component analysis\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=20)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.fit_transform(X_test)\n",
    "    \n",
    "   \n",
    "    \t\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression(random_state = 0)))\n",
    "    models.append(('SVC', SVC(kernel = 'rbf', random_state = 0)))\n",
    "     \n",
    "    models.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\n",
    "    models.append(('DT', DecisionTreeClassifier(random_state = 0)))\n",
    "    \n",
    "    scoring = 'accuracy'\n",
    "    \n",
    "    # Cross Validate\n",
    "    results = []\n",
    "    names = []\n",
    "    timer = []\n",
    "    print('Model | Mean of CV | Std. Dev. of CV | Time')\n",
    "    for name, model in models:\n",
    "        start_time = time.time()\n",
    "        kfold = model_selection.KFold(n_splits=5, random_state=0)\n",
    "        cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        t = (time.time() - start_time)\n",
    "        timer.append(t)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f) %f s\" % (name, cv_results.mean(), cv_results.std(), t)\n",
    "        print(msg)\n",
    "    \n",
    "        \n",
    "    models = []\n",
    "    for i in range(1,41): \n",
    "        models.append(('KNN', KNeighborsClassifier(n_neighbors=i)))\n",
    "        results = []\n",
    "        names = []\n",
    "        timer = []\n",
    "        cv_knn = []\n",
    "        \n",
    "        print('Model  | Mean of CV | Std. Dev. of CV | Time',i)\n",
    "        for name, model in models:\n",
    "            start_time = time.time()\n",
    "            kfold = model_selection.KFold(n_splits=4, random_state=42)\n",
    "            cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "            t = (time.time() - start_time)\n",
    "            timer.append(t)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "            msg = \"%s: %f (%f) %f s\" % (name, cv_results.mean(), cv_results.std(), t)\n",
    "                \n",
    "            cv_knn.append(cv_results.mean())\n",
    "        print(msg)\n",
    "        \n",
    "    print('\\nmaximum accuracy for valence is',max(cv_knn))\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  \n",
    "    plt.plot(range(1, 41), cv_knn, color='red', linestyle='dashed', marker='o',  \n",
    "             markerfacecolor='blue', markersize=10)\n",
    "    plt.title('mean cv_results of  K Value for Valence')  \n",
    "    plt.xlabel('K Value')  \n",
    "    plt.ylabel('Mean cv_result')  \n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    cross_validate0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_visual():\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from pandas.plotting import scatter_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    features = pd.read_csv('features.csv') \n",
    "    labels = pd.read_csv('labels.csv')\n",
    "\n",
    "    names = ['valence', 'arousal', 'dominance', 'liking']\n",
    "    dataset =features.transpose().reindex()\n",
    "\n",
    "\n",
    "# Summary of dataset\n",
    "    print('Summary of dataset\\n')\n",
    "# shape\n",
    "    print('1. Shape is:')\n",
    "    print(dataset.shape)\n",
    "# head\n",
    "    print('\\n2. First 8 rows are as:')\n",
    "    print(dataset.head(8))\n",
    "# descriptions\n",
    "    print('\\n3. Statistical description:')\n",
    "    print(dataset.describe())\n",
    "    types = dataset.dtypes\n",
    "    print('\\n4. Data Types:')\n",
    "    print(types)\n",
    "\n",
    "# Data visualisation\n",
    "# box and whisker plots\n",
    "    for i in range(5):\n",
    "        sns.set_style('whitegrid')\n",
    "        sns.boxplot(dataset[i])\n",
    "        plt.title('Magnitude versus ' + str(i+1) + ' channel')\n",
    "        plt.show()\n",
    "    \n",
    "    import seaborn as sns\n",
    "    from matplotlib import pyplot as plt\n",
    "    for i in range(5):\n",
    "        sns.distplot(dataset[i])\n",
    "        plt.title('Magnitude versus ' + str(i+1) + ' channel')\n",
    "        plt.show()\n",
    "if __name__ == '__main__':\n",
    "   data_visual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae36c89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 43  48]\n",
      " [ 63 102]]\n",
      "Accuracy score of Valence \n",
      "56.640625\n",
      "[[ 30  60]\n",
      " [ 52 114]]\n",
      "Accuracy score of Arousal \n",
      "56.25\n",
      "[[ 14  62]\n",
      " [ 21 159]]\n",
      "Accuracy score of Dominance \n",
      "67.578125\n",
      "[[  1  78]\n",
      " [ 11 166]]\n",
      "Accuracy score of Liking \n",
      "65.234375\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def knn_classifier():\n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_0.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # KNN\n",
    "    clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of Valence \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###############################################################################\n",
    "    \n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_1.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # KNN\n",
    "    clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of Arousal \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "    \n",
    "    ###############################################################################\n",
    "    \n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_2.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimit                      \n",
    "                         \n",
    "                         \n",
    "                         \n",
    "                         \n",
    "                         er=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # KNN\n",
    "    clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of Dominance \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "    \n",
    "    ###############################################################################\n",
    "    \n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_3.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # KNN\n",
    "    clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of Liking \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    knn_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9048478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 234]\n",
      " [  0 406]]\n",
      "Accuracy score of Valence \n",
      "63.4375\n",
      "[[  4 229]\n",
      " [  2 405]]\n",
      "Accuracy score of Arousal \n",
      "63.90625\n",
      "[[  0 207]\n",
      " [  1 432]]\n",
      "Accuracy score of Dominance \n",
      "67.5\n",
      "[[  1 209]\n",
      " [  0 430]]\n",
      "Accuracy score of liking  \n",
      "67.34375\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def svm_classifier(): \n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_0.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # SVM Classifier\n",
    "    clf = SVC(kernel = 'rbf', random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of Valence \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "    ###############################################################\n",
    "    \n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_1.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # SVM Classifier\n",
    "    clf = SVC(kernel = 'rbf', random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of Arousal \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "    \n",
    "    ####################################################################\n",
    "    \n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_2.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # SVM Classifier\n",
    "    clf = SVC(kernel = 'rbf', random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of Dominance \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "    ######################################################################3\n",
    "    \n",
    "    file_x = 'D:/Major Project/data/features_sampled.dat'\n",
    "    file_y = 'D:/Major Project/data/label_class_3.dat'\n",
    "    \n",
    "    X = numpy.genfromtxt(file_x, delimiter=' ')\n",
    "    y = numpy.genfromtxt(file_y, delimiter=' ')\n",
    "    \n",
    "    # Split the data into training/testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \t\n",
    "    # SVM Classifier\n",
    "    clf = SVC(kernel = 'rbf', random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    print(cm)\n",
    "    print(\"Accuracy score of liking  \")\n",
    "    print(accuracy_score(y_test, y_predict)*100)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    svm_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8a43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
